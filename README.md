# DevOps Lab

Репозиторий содержит наработки и эксперименты по DevOps-инфраструктуре.  

Цель данного репозитория - **отображение структуры и документации**. Рабочий репозиторий, используемый в CI/CD процессах, развернут на self-hosted Gitea.

> ⚠️ **Примечание:** Текущие планы и задачи можно посмотреть в Issues.


Посмотреть развернутый кластер можно по ссылкам:  
- https://lab-sarychev.ru - описание стенда  
- https://app.lab-sarychev.ru - тестовое приложение внутри кластера
> ⚠️ **Примечание:** На данный момент у проекта нет разделения на dev, stage и prod среды, поэтому при выполнении работ некоторые части стенда могут быть временно недоступны.
- https://grafana.lab-sarychev.ru - мониторинг кластера  

---

## Компоненты

- **Nodes** – содержит описание машин, docker-compose конфигурации, скрипты и Ansible плейбуки для настройки узлов под нужные инструменты.
- **Helm Charts** – описание чартов, используемых в кластере.
- **Jenkins** – содержит примеры Jenkinsfile и pipeline конфигураций.
- **Ansible** – плейбуки, используемые в CI/CD пайплайнах.
- **Test Application** – код тестового приложения-заглушки, разворачиваемого внутри кластера.

---

## Kubernetes кластер

Кластер развернут на ряде виртуальных машин под управлением Proxmox (всего на двух физических узлах):

- **Мини-ПК Beelink SER5 (AMD Ryzen 7 5800H CPU @ 4.30GHz | Mem: 32Gi DDR4):**
  - control-plane 1-2 – одна основная и одна резервная нода для управления кластером.

- **Стационарный ПК (Intel(R) Core(TM) i5-4460  CPU @ 3.20GHz | Mem: 16Gi DDR3):**
  - worker-node 1-3 – три ноды, на которых крутится тестовое приложение.

> ⚠️ Обе control-plane ноды временно живут на одном физическом узле, так как машина под worker-ноды обладает ограниченными ресурсами.  

> ⚠️ В тестовых сценариях допустимо кратковременное добавление нод или их отключение для проверки устойчивости инфраструктуры.

---

### Приложения в кластере

Кластер содержит несколько приложений, разложенных по собственным пространствам имён:

- **traefik** – ingress-контроллер, дающий доступ к приложениям через worker-ноды.  
  Балансировка нагрузки происходит через HAProxy, развернутой на роутере под управлением openWRT, служащей точкой входа для клиентов из внешней сети.

- **longhorn** – обеспечивает устойчивый доступ к данным Persistent Volume, на котором хранятся записи MongoDB тестового приложения.

- **monitoring:**
  - `kube-state-metrics` – метрики кластера для внешнего Prometheus.
  - `promtail` – передача логов на внешний Loki.
  - `node-exporter` – метрики физических нод кластера.

- **app-stack (тестовое приложение):**
  - `backend (deployment|service)` – деплоймент на 2 пода и сервис для сетевого доступа; выполняет запросы к БД и отдаёт их на фронт.
  - `compute-app (deployment)` – масштабируемый деплоймент (по умолчанию на 3 пода), нагружает worker-ноды имитацией вычислений и пишет записи в БД.
  - `frontend (deployment|service)` – отображает записи, которые compute-app пишет в БД.
  - `app-ingress` – объект Ingress для backend и frontend через Traefik ingress controller.
  - `MongoDB (ConfigMap|Deployment|PersistentVolumeClaim|Service)` – база данных для записей compute-app; ConfigMap используется для конфигурации, а PVC для хранения данных.
---
### Используемые Jenkins пайплайны
 - `Jenkinsfile.create-vms` - создает ВМ под будущие задачи;
 - `Jenkinsfile.create-k8s-master` - создает ВМ на выбранном узле, устанавливает Kubernetes на созданной машине и инициализирует его как control-plane;  
 - `Jenkinsfile.create-k8s-nodes` - создает комплект ВМ на выбранном узле и подключает их к необходимому кластеру k8s;  
 - `Jenkinsfile.deploy-app` - деплоит тестовое приложение в кластере;
 - `Jenkinsfile.deploy-components` - деплоит инфраструктурные компоненты в кластере;
 - `Jenkinsfile.environment-creation-pipeline` - мета-пайплайн по созданию dev/test/prod среды с нуля.
 
> ⚠️ Создание ВМ на данный момент происходит с помощью Ansible, а не Terraform, поскольку второй на данный момент недоступен в РФ и обходные пути для его использования будут использованы в будущем.
