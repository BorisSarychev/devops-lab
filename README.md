# DevOps Lab

Репозиторий содержит наработки и эксперименты по DevOps-инфраструктуре.  

Цель данного репозитория - **отображение структуры и документации**, f не полностью рабочей инфраструктуры.  
Рабочий репозиторий, используемый в CI/CD процессах, развернут на self-hosted GitLab.

> ⚠️ **Примечание:** Файлы в этом репозитории могут быть частично пустыми или содержать ссылки на локальные ресурсы.

> ⚠️ **Примечание:** Данный репозиторий и демонстрационный стенд ещё находятся в разработке, первый релиз ожидается 30.12.2025. Текущие задачи можно посмотреть в Issues.

Посмотреть развернутый кластер можно по ссылкам:  
- https://lab-sarychev.ru - описание стенда  
- https://app.lab-sarychev.ru - тестовое приложение внутри кластера  
- https://grafana.lab-sarychev.ru - мониторинг кластера  

---

## Компоненты

- **VM** – содержит docker-compose конфигурации, скрипты и Ansible плейбуки для настройки виртуальных машин под нужные инструменты.
- **Helm Charts** – описание чартов, используемых в кластере.
- **Jenkins** – содержит примеры Jenkinsfile и pipeline конфигураций.
- **Ansible** – плейбуки, используемые в CI/CD пайплайнах.
- **Test Application** – код тестового приложения, разворачиваемого внутри кластера.

---

## Kubernetes кластер

Кластер развернут на ряде виртуальных машин под управлением Proxmox (всего на двух физических узлах):

- **Мини-ПК Beelink SER5 (AMD Ryzen 7 5800H | Mem: 27Gi DDR4):**
  - control-plane 1-2 – одна основная и одна резервная нода для управления кластером.

- **Стационарный ПК (Intel(R) Core(TM) i5-4460  CPU @ 3.20GHz | Mem: 15Gi DDR3):**
  - worker-node 1-3 – три ноды, на которых крутится тестовое приложение.

> ⚠️ Обе control-plane ноды временно живут на одном физическом узле, так как машина под worker-ноды обладает ограниченными ресурсами.  
> Диверсификация нод будет произведена при сборке адекватной серверной машины.

> ⚠️ В тестовых сценариях допустимо кратковременное добавление нод или их отключение для проверки устойчивости инфраструктуры.

---

### Приложения в кластере

Кластер содержит несколько приложений, разложенных по пространствам имён:

- **traefik | traefik** – ingress-контроллер, дающий доступ к приложениям через worker-ноды.  
  Балансировка нагрузки происходит через HAProxy, развернутый на отдельной VM, служащей точкой входа для клиентов из внешней сети.

- **longhorn | longhorn** – обеспечивает устойчивый доступ к данным Persistent Volume, на котором хранятся записи MongoDB тестового приложения.

- **monitoring:**
  - `kube-state-metrics` – метрики кластера для внешнего Prometheus.
  - `promtail` – передача логов на внешний Loki.
  - `node-exporter` – метрики физических нод кластера.

- **app-stack (тестовое приложение):**
  - `backend (deployment|service)` – деплоймент на 2 пода и сервис для сетевого доступа; выполняет запросы к БД и отдаёт их на фронт.
  - `compute-app (deployment)` – масштабируемый деплоймент (по умолчанию на 3 пода), нагружает worker-ноды имитацией вычислений и пишет записи в БД.
  - `frontend (deployment|service)` – отображает записи, которые compute-app пишет в БД.
  - `app-ingress` – объект Ingress для backend и frontend через Traefik ingress controller.
  - `MongoDB (ConfigMap|Deployment|PersistentVolumeClaim|Service)` – база данных для записей compute-app; ConfigMap используется для конфигурации, а PVC для хранения данных.
---
### Используемые Jenkins пайплайны  
 - `Jenkinsfile.create-k8s-master` - создает ВМ в качестве control-plane под кластер k8s и инициализирует его;  
 - `Jenkinsfile.create-k8s-nodes` - создает комплект ВМ и подключает их к необходимой control-plane;  
 - `Jenkinsfile.deploy-devops-showcase` - деплоит тестовое приложение в кластере, включая инфраструктурные элементы вроде longhorn или traefik (в зависимости от выбранного кластера - prod или dev)  
> ⚠️ Создание ВМ на данный момент происходит с помощью Ansible, а не Terraform, поскольку второй на данный момент недоступен в РФ и обходные пути для его использования будут использованы когда стенд будет завершен.
